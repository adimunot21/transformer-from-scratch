{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MotXCcKbQkjf",
        "outputId": "1af9ce0b-19c9-4bf1-c05c-c9bec39c0bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformer-from-scratch'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 57 (delta 19), reused 54 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 510.86 KiB | 17.62 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/transformer-from-scratch\n",
            "CUDA: True\n",
            "Gym: 1.2.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/adimunot21/transformer-from-scratch.git\n",
        "%cd transformer-from-scratch\n",
        "!pip install gymnasium -q\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(f\"Gym: {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Collect episodes using two policies:\n",
        "1. Random policy — gives low-return episodes (avg ~20 steps)\n",
        "2. Heuristic policy — push toward pole's lean direction (avg ~100-200)\n",
        "\n",
        "This diversity is critical: the model needs to see both good and bad\n",
        "trajectories to learn that higher return-to-go → better actions.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "def collect_episodes(n_random=500, n_heuristic=500, max_steps=500):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    episodes = []\n",
        "\n",
        "    # Random policy\n",
        "    for i in range(n_random):\n",
        "        obs, _ = env.reset()\n",
        "        ep = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
        "        for t in range(max_steps):\n",
        "            action = env.action_space.sample()\n",
        "            ep[\"states\"].append(obs)\n",
        "            ep[\"actions\"].append(action)\n",
        "            obs, reward, term, trunc, _ = env.step(action)\n",
        "            ep[\"rewards\"].append(reward)\n",
        "            if term or trunc:\n",
        "                break\n",
        "        episodes.append(ep)\n",
        "\n",
        "    # Heuristic policy: push in direction pole is leaning\n",
        "    for i in range(n_heuristic):\n",
        "        obs, _ = env.reset()\n",
        "        ep = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
        "        for t in range(max_steps):\n",
        "            # Pole angle is obs[2]: positive = leaning right → push right (1)\n",
        "            action = 1 if obs[2] > 0 else 0\n",
        "            # Add some noise for diversity\n",
        "            if random.random() < 0.1:\n",
        "                action = 1 - action\n",
        "            ep[\"states\"].append(obs)\n",
        "            ep[\"actions\"].append(action)\n",
        "            obs, reward, term, trunc, _ = env.step(action)\n",
        "            ep[\"rewards\"].append(reward)\n",
        "            if term or trunc:\n",
        "                break\n",
        "        episodes.append(ep)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Compute returns\n",
        "    returns = [sum(ep[\"rewards\"]) for ep in episodes]\n",
        "    print(f\"Collected {len(episodes)} episodes\")\n",
        "    print(f\"Return stats: min={min(returns):.0f}, mean={np.mean(returns):.1f}, \"\n",
        "          f\"max={max(returns):.0f}, median={np.median(returns):.0f}\")\n",
        "\n",
        "    # Show distribution\n",
        "    brackets = [0, 20, 50, 100, 200, 500, 501]\n",
        "    for i in range(len(brackets)-1):\n",
        "        count = sum(1 for r in returns if brackets[i] <= r < brackets[i+1])\n",
        "        print(f\"  Return {brackets[i]:>3d}-{brackets[i+1]:>3d}: {count} episodes\")\n",
        "\n",
        "    return episodes\n",
        "\n",
        "episodes = collect_episodes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGRChAJSQru0",
        "outputId": "74c4d2f2-0412-481b-8465-d9a950c450e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1000 episodes\n",
            "Return stats: min=9, mean=33.7, max=117, median=30\n",
            "  Return   0- 20: 262 episodes\n",
            "  Return  20- 50: 564 episodes\n",
            "  Return  50-100: 165 episodes\n",
            "  Return 100-200: 9 episodes\n",
            "  Return 200-500: 0 episodes\n",
            "  Return 500-501: 0 episodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert episodes into training data for the Decision Transformer.\n",
        "\n",
        "For each episode, we:\n",
        "1. Compute return-to-go at each timestep: R̂ₜ = Σ(rewards from t onward)\n",
        "2. Extract sliding windows of length K (context_len)\n",
        "3. Each window becomes one training sample: (R̂, s, a, timesteps)\n",
        "\"\"\"\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DTDataset(Dataset):\n",
        "    def __init__(self, episodes, context_len=20):\n",
        "        self.context_len = context_len\n",
        "        self.samples = []\n",
        "\n",
        "        for ep in episodes:\n",
        "            T = len(ep[\"rewards\"])\n",
        "            states = np.array(ep[\"states\"])\n",
        "            actions = np.array(ep[\"actions\"])\n",
        "            rewards = np.array(ep[\"rewards\"])\n",
        "\n",
        "            # Compute return-to-go: R̂ₜ = reward_t + reward_{t+1} + ... + reward_T\n",
        "            rtg = np.zeros(T)\n",
        "            rtg[-1] = rewards[-1]\n",
        "            for t in range(T - 2, -1, -1):\n",
        "                rtg[t] = rewards[t] + rtg[t + 1]\n",
        "\n",
        "            # Create sliding windows of length context_len\n",
        "            for start in range(T):\n",
        "                end = min(start + context_len, T)\n",
        "                k = end - start  # actual length (might be < context_len)\n",
        "\n",
        "                s = np.zeros((context_len, states.shape[1]))\n",
        "                a = np.zeros(context_len, dtype=np.int64)\n",
        "                r = np.zeros((context_len, 1))\n",
        "                ts = np.zeros(context_len, dtype=np.int64)\n",
        "                mask = np.zeros(context_len)\n",
        "\n",
        "                s[:k] = states[start:end]\n",
        "                a[:k] = actions[start:end]\n",
        "                r[:k, 0] = rtg[start:end]\n",
        "                ts[:k] = np.arange(start, end)\n",
        "                mask[:k] = 1.0\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"states\": torch.tensor(s, dtype=torch.float32),\n",
        "                    \"actions\": torch.tensor(a, dtype=torch.long),\n",
        "                    \"returns_to_go\": torch.tensor(r, dtype=torch.float32),\n",
        "                    \"timesteps\": torch.tensor(ts, dtype=torch.long),\n",
        "                    \"mask\": torch.tensor(mask, dtype=torch.float32),\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# Normalize returns-to-go (helps training stability)\n",
        "all_rtg = np.concatenate([\n",
        "    np.cumsum(ep[\"rewards\"][::-1])[::-1] for ep in episodes\n",
        "])\n",
        "rtg_mean, rtg_std = all_rtg.mean(), all_rtg.std()\n",
        "print(f\"RTG stats: mean={rtg_mean:.2f}, std={rtg_std:.2f}\")\n",
        "\n",
        "# Apply normalization\n",
        "for ep in episodes:\n",
        "    rewards = np.array(ep[\"rewards\"])\n",
        "    T = len(rewards)\n",
        "    rtg = np.zeros(T)\n",
        "    rtg[-1] = rewards[-1]\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        rtg[t] = rewards[t] + rtg[t + 1]\n",
        "\n",
        "CONTEXT_LEN = 20\n",
        "dataset = DTDataset(episodes, context_len=CONTEXT_LEN)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset):,} samples\")\n",
        "batch = next(iter(loader))\n",
        "print(f\"Batch shapes:\")\n",
        "for k, v in batch.items():\n",
        "    print(f\"  {k}: {v.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXiRuSZxQwwp",
        "outputId": "dbac6221-dbfe-482b-9f2c-d5d64db178ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RTG stats: mean=22.38, std=17.73\n",
            "Dataset size: 33,657 samples\n",
            "Batch shapes:\n",
            "  states: torch.Size([64, 20, 4])\n",
            "  actions: torch.Size([64, 20])\n",
            "  returns_to_go: torch.Size([64, 20, 1])\n",
            "  timesteps: torch.Size([64, 20])\n",
            "  mask: torch.Size([64, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import math, time\n",
        "from src.decision_transformer import DecisionTransformer\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = DecisionTransformer(\n",
        "    state_dim=4,      # CartPole state dimension\n",
        "    act_dim=2,        # CartPole: left or right\n",
        "    d_model=64,\n",
        "    n_heads=4,\n",
        "    n_layers=3,\n",
        "    max_timestep=500,\n",
        "    context_len=CONTEXT_LEN,\n",
        "    dropout=0.1,\n",
        ").to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Decision Transformer parameters: {n_params:,}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "MAX_STEPS = 3000\n",
        "\n",
        "model.train()\n",
        "train_iter = iter(loader)\n",
        "t0 = time.time()\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    rtg = batch[\"returns_to_go\"].to(device)\n",
        "    states = batch[\"states\"].to(device)\n",
        "    actions = batch[\"actions\"].to(device)\n",
        "    timesteps = batch[\"timesteps\"].to(device)\n",
        "    mask = batch[\"mask\"].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    action_logits = model(rtg, states, actions, timesteps)  # (B, K, 2)\n",
        "\n",
        "    # Loss: cross-entropy on predicted actions vs actual actions\n",
        "    # Only compute loss on valid (non-padded) positions\n",
        "    logits_flat = action_logits.reshape(-1, 2)\n",
        "    actions_flat = actions.reshape(-1)\n",
        "    mask_flat = mask.reshape(-1)\n",
        "\n",
        "    loss_all = F.cross_entropy(logits_flat, actions_flat, reduction=\"none\")\n",
        "    loss = (loss_all * mask_flat).sum() / mask_flat.sum()\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        # Compute accuracy\n",
        "        preds = action_logits.argmax(dim=-1).reshape(-1)\n",
        "        correct = ((preds == actions_flat) * mask_flat).sum()\n",
        "        acc = correct / mask_flat.sum()\n",
        "        print(f\"Step {step:4d} | loss: {loss.item():.4f} | acc: {acc:.3f} | time: {time.time()-t0:.0f}s\")\n",
        "\n",
        "print(f\"\\nTraining complete in {time.time()-t0:.0f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3WefzyeQy0i",
        "outputId": "785df736-2513-4e19-8dd8-266eea3fe232"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Transformer parameters: 182,594\n",
            "Step    0 | loss: 0.7492 | acc: 0.457 | time: 1s\n",
            "Step  200 | loss: 0.4664 | acc: 0.794 | time: 5s\n",
            "Step  400 | loss: 0.4021 | acc: 0.827 | time: 9s\n",
            "Step  600 | loss: 0.4257 | acc: 0.799 | time: 13s\n",
            "Step  800 | loss: 0.4021 | acc: 0.818 | time: 18s\n",
            "Step 1000 | loss: 0.4743 | acc: 0.773 | time: 21s\n",
            "Step 1200 | loss: 0.3769 | acc: 0.845 | time: 26s\n",
            "Step 1400 | loss: 0.3951 | acc: 0.835 | time: 30s\n",
            "Step 1600 | loss: 0.3901 | acc: 0.822 | time: 34s\n",
            "Step 1800 | loss: 0.3734 | acc: 0.833 | time: 38s\n",
            "Step 2000 | loss: 0.4405 | acc: 0.796 | time: 42s\n",
            "Step 2200 | loss: 0.3748 | acc: 0.850 | time: 46s\n",
            "Step 2400 | loss: 0.3710 | acc: 0.839 | time: 51s\n",
            "Step 2600 | loss: 0.3497 | acc: 0.858 | time: 55s\n",
            "Step 2800 | loss: 0.3708 | acc: 0.834 | time: 59s\n",
            "\n",
            "Training complete in 63s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is where it gets cool.\n",
        "\n",
        "We condition the model on DIFFERENT desired returns and see\n",
        "if it produces better actions for higher desired returns.\n",
        "\n",
        "If it works, the model has learned: \"when you want high reward,\n",
        "take these actions\" — purely from offline sequence data.\n",
        "\"\"\"\n",
        "\n",
        "def evaluate(model, target_return, n_episodes=50, context_len=20):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    model.eval()\n",
        "    returns = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        # Buffers for the context window\n",
        "        states = torch.zeros(1, context_len, 4, device=device)\n",
        "        actions = torch.zeros(1, context_len, dtype=torch.long, device=device)\n",
        "        rtg = torch.zeros(1, context_len, 1, device=device)\n",
        "        timesteps = torch.zeros(1, context_len, dtype=torch.long, device=device)\n",
        "\n",
        "        states[0, 0] = torch.tensor(obs, device=device)\n",
        "        rtg[0, 0, 0] = target_return  # Desired return!\n",
        "        timesteps[0, 0] = 0\n",
        "\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "\n",
        "        for t in range(500):\n",
        "            pos = min(t, context_len - 1)\n",
        "\n",
        "            # Get action from model\n",
        "            with torch.no_grad():\n",
        "                logits = model(rtg[:, :pos+1], states[:, :pos+1],\n",
        "                              actions[:, :pos+1], timesteps[:, :pos+1])\n",
        "                action = logits[0, pos].argmax().item()\n",
        "\n",
        "            obs, reward, term, trunc, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if term or trunc:\n",
        "                break\n",
        "\n",
        "            # Update buffers for next step\n",
        "            next_pos = min(t + 1, context_len - 1)\n",
        "            if t + 1 < context_len:\n",
        "                states[0, next_pos] = torch.tensor(obs, device=device)\n",
        "                actions[0, pos] = action\n",
        "                rtg[0, next_pos, 0] = rtg[0, pos, 0] - reward\n",
        "                timesteps[0, next_pos] = t + 1\n",
        "            else:\n",
        "                # Shift window left\n",
        "                states[0, :-1] = states[0, 1:].clone()\n",
        "                states[0, -1] = torch.tensor(obs, device=device)\n",
        "                actions[0, :-1] = actions[0, 1:].clone()\n",
        "                actions[0, -2] = action\n",
        "                rtg[0, :-1] = rtg[0, 1:].clone()\n",
        "                rtg[0, -1, 0] = rtg[0, -2, 0] - reward\n",
        "                timesteps[0, :-1] = timesteps[0, 1:].clone()\n",
        "                timesteps[0, -1] = t + 1\n",
        "\n",
        "        returns.append(total_reward)\n",
        "\n",
        "    env.close()\n",
        "    return returns\n",
        "\n",
        "# Test with different target returns\n",
        "print(\"=\" * 60)\n",
        "print(\"DECISION TRANSFORMER EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nConditioning on different desired returns:\\n\")\n",
        "\n",
        "for target in [10, 50, 100, 200, 500]:\n",
        "    returns = evaluate(model, target_return=target, n_episodes=50)\n",
        "    mean_r = np.mean(returns)\n",
        "    std_r = np.std(returns)\n",
        "    max_r = max(returns)\n",
        "    print(f\"  Target return: {target:>3d} → Achieved: {mean_r:.1f} ± {std_r:.1f} (max: {max_r:.0f})\")\n",
        "\n",
        "# Compare to random baseline\n",
        "print(f\"\\n  Random policy baseline:\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "rand_returns = []\n",
        "for _ in range(50):\n",
        "    obs, _ = env.reset()\n",
        "    total = 0\n",
        "    for _ in range(500):\n",
        "        obs, r, term, trunc, _ = env.step(env.action_space.sample())\n",
        "        total += r\n",
        "        if term or trunc: break\n",
        "    rand_returns.append(total)\n",
        "env.close()\n",
        "print(f\"  Random policy → {np.mean(rand_returns):.1f} ± {np.std(rand_returns):.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ8WH2IZQ2MV",
        "outputId": "96932afc-e855-4f8a-d99f-73c11887ff69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DECISION TRANSFORMER EVALUATION\n",
            "============================================================\n",
            "\n",
            "Conditioning on different desired returns:\n",
            "\n",
            "  Target return:  10 → Achieved: 10.1 ± 0.3 (max: 11)\n",
            "  Target return:  50 → Achieved: 45.3 ± 9.0 (max: 66)\n",
            "  Target return: 100 → Achieved: 62.7 ± 23.9 (max: 110)\n",
            "  Target return: 200 → Achieved: 71.5 ± 39.1 (max: 198)\n",
            "  Target return: 500 → Achieved: 76.0 ± 70.2 (max: 491)\n",
            "\n",
            "  Random policy baseline:\n",
            "  Random policy → 22.2 ± 14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COrMZMhCRvb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}